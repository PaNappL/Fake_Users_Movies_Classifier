{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, recall_score, precision_score\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import numpy as np\n",
    "\n",
    "from datasetStuff import expandDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('./model_checkpoints', ignore_errors=True)\n",
    "\n",
    "X = df_final\n",
    "y = labels\n",
    "\n",
    "input_shape = 600\n",
    "pca_test = PCA(n_components=input_shape)\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.235, random_state=random_state, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_val, y_val, test_size=0.5, random_state=random_state, stratify=y_val)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled, X_val_scaled, X_test_scaled = scaler.fit_transform(X_train), scaler.transform(X_val), scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled, X_val_scaled, X_test_scaled = pca_test.fit_transform(X_train_scaled), pca_test.transform(X_val_scaled), pca_test.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "detector = IsolationForest(contamination = 0.1).fit(X_val_scaled)\n",
    "pred_anomalies = detector.predict(X_val_scaled)\n",
    "X_val_scaled = X_val_scaled[pred_anomalies == 1]\n",
    "y_val = y_val[pred_anomalies == 1]\n",
    "\n",
    "oversample = SMOTE()\n",
    "X_val_scaled, y_val = oversample.fit_resample(X_val_scaled, y_val)\n",
    "\n",
    "zero_weight = (np.sum(y_train==1)/np.sum(y_train==0))*1.7\n",
    "\n",
    "#Create a simple neural network model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(100, activation='gelu',\n",
    "              input_shape=(input_shape,),\n",
    "              kernel_regularizer=tf.keras.regularizers.L1(0.0012),\n",
    "              activity_regularizer=tf.keras.regularizers.L2(0.0003),\n",
    "              kernel_initializer=tf.keras.initializers.GlorotUniform()\n",
    "              ),\n",
    "    tf.keras.layers.Dropout(0.7),\n",
    "    tf.keras.layers.Dense(70, activation='gelu',\n",
    "              kernel_regularizer=tf.keras.regularizers.L1(0.0012),\n",
    "              activity_regularizer=tf.keras.regularizers.L2(0.0003),\n",
    "              kernel_initializer=tf.keras.initializers.GlorotUniform()\n",
    "              ),\n",
    "    tf.keras.layers.Dropout(0.7),\n",
    "    tf.keras.layers.Dense(2, activation='sigmoid',\n",
    "              kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
    "                          ),\n",
    "])\n",
    "\n",
    "# Convert target labels to one-hot encoding\n",
    "y_train_categorized, y_val_categorized = to_categorical(y_train, num_classes=2), to_categorical(y_val, num_classes=2)\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n",
    "\n",
    "# Define a callback to save checkpoints\n",
    "checkpoint_filepath = \"model_checkpoints/checkpoint_{epoch:02d}.h5\"\n",
    "\n",
    "# Save a model checkpoint every 100 epochs\n",
    "model_save = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    period=2,  # Save every 100 epochs\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-4),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.AUC()])\n",
    "epochs = 200\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train_categorized, epochs=epochs, batch_size=32, class_weight={0:1, 1:1}, validation_data=(X_val_scaled, y_val_categorized), callbacks=[early_stopping, model_save])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
