{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Select path to current folder and split by \\\\\n",
    "main_path = sys.path[0].split(\"\\\\\")\n",
    "\n",
    "# Asssign path to parent folder\n",
    "# path_to_parent allows access to any folder from within parent folder, no matter the location of this file within the parent folder\n",
    "# i.e.: Don't need to specify \"../\" x amount of times\n",
    "path_to_parent = []\n",
    "for element in main_path:\n",
    "    path_to_parent.append(element)\n",
    "    if \"Fake_Users_Movies_Classifier\" == element:\n",
    "        break\n",
    "\n",
    "path_to_parent = \"\\\\\".join(path_to_parent)\n",
    "\n",
    "# Add path to feature generation folder\n",
    "sys.path.append(path_to_parent+\"\\\\feature_generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import feature generator for week 4\n",
    "from feature_gen_wk4 import feature_gen\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize a list with file names\n",
    "# Clarification:\n",
    "#       The file names are only the first word within the file name, as this is the only difference between all files\n",
    "file_names = [\"first\",\"second\",\"third\", \"fourth\"]\n",
    "# Create and initialize a feature generation class\n",
    "feature_generator = feature_gen()\n",
    "\n",
    "# Initialize a variable which will be used to store the entire dataset\n",
    "df_final = None\n",
    "\n",
    "# Loop through the filee names and generate dataframe with features\n",
    "for name in file_names:\n",
    "    # Create string path to labelled data\n",
    "    path_to_file = path_to_parent + f\"/data/labelled_data/{name}_batch_with_labels_likes.npz\"\n",
    "    # Generate features from file\n",
    "    df = feature_generator.retrieveAndGenerate(path_to_file)\n",
    "\n",
    "    # Check if df_final is not a NoneType (therefore does not have any data inside)\n",
    "    if type(df_final) != None:\n",
    "        # If no, concatenate the two dataframes\n",
    "        df_final = pd.concat([df_final, df]).reset_index(drop=True)\n",
    "    else:\n",
    "        # If yes, assign the generated dataframe to df_final\n",
    "        df_final = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Retrieve labels and assign to y\n",
    "y = df_final['label']\n",
    "# Remove labels and users from dataset and assign to x\n",
    "X = df_final.drop(['user','label'],axis=1)\n",
    "\n",
    "# Set the value of the amount of features which will be input to the model\n",
    "input_shape = len(X.columns)\n",
    "\n",
    "# Set the random state for train_test_split\n",
    "random_state = 42\n",
    "\n",
    "# Split the data into 3 set: train, validation and test with ratios: 0.765 : 0.117 : 0.117 respectively\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.235, random_state=random_state, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_val, y_val, test_size=0.5, random_state=random_state, stratify=y_val)\n",
    "\n",
    "# Apply scaler on train, validation and test sets\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled, X_val_scaled, X_test_scaled = scaler.fit_transform(X_train), scaler.transform(X_val), scaler.transform(X_test)\n",
    "\n",
    "# Calculate the ratio of 1 labels (fake) to 0 labels (non-fake) and multiply by a set amount\n",
    "# This value will be used as the weight applied to the 0 label during weighting to prevent label bias\n",
    "zero_weight = (np.sum(y_train==1)/np.sum(y_train==0))*1.5\n",
    "\n",
    "# Create model using tensorflow sequential class\n",
    "model = tf.keras.Sequential([\n",
    "    # Initialize first layer using GlorotUniform, assign 300 neurons and set regularizers to optimal value\n",
    "    # In this implementation ReLU activation was used as it performed the best, based on internal testing\n",
    "    tf.keras.layers.Dense(300, activation='relu',\n",
    "              input_shape=(input_shape,),\n",
    "              kernel_regularizer=tf.keras.regularizers.L1(0.001),\n",
    "              activity_regularizer=tf.keras.regularizers.L2(0.0015),\n",
    "              kernel_initializer=tf.keras.initializers.GlorotUniform()\n",
    "              ),\n",
    "    # Apply Dropout with 0.5 dropout rate to prevent overfitting\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    # Initialize output layer using GlorotUniform, with sigmoid activation\n",
    "    tf.keras.layers.Dense(2, activation='sigmoid',\n",
    "              kernel_initializer=tf.keras.initializers.GlorotUniform(),\n",
    "                          ),\n",
    "])\n",
    "\n",
    "# Convert target labels to one-hot encoding\n",
    "y_train_categorized, y_val_categorized = to_categorical(y_train, num_classes=2), to_categorical(y_val, num_classes=2)\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "# Define a callback to save checkpoints\n",
    "checkpoint_filepath = path_to_parent + \"/model_checkpoints/wk4/checkpoint_{epoch:1d}.h5\"\n",
    "\n",
    "# Save a model checkpoint every 2 epochs\n",
    "model_save = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    period=2,  # Save every 2 epochs\n",
    ")\n",
    "\n",
    "# Compile the model with Adam optimizer with a learning rate of 0.0004\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=4e-4),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=[tf.keras.metrics.AUC()])\n",
    "# Set the amount of epochs to train for\n",
    "epochs = 20\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train_categorized, epochs=epochs, batch_size=32, class_weight={0:zero_weight, 1:1}, validation_data=(X_val_scaled, y_val_categorized), callbacks=[early_stopping, model_save])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load class with getScores function for calulating predictions scores\n",
    "from feature_selection import selectors\n",
    "\n",
    "# Initialize list for storing model prediction scores\n",
    "scores = []\n",
    "\n",
    "try:\n",
    "  # Loop through all model checkpoints\n",
    "  # Start = 0\n",
    "  # End = Number of epochs / epoch_save_period + 1\n",
    "  for i in range(1,int(epochs/2)+1):\n",
    "    path = path_to_parent + f'/model_checkpoints/wk4/checkpoint_{i*2}.h5'\n",
    "\n",
    "    # Load model weights from path and predict test set labels\n",
    "    model.load_weights(path)\n",
    "    y_pred = model.predict(X_test_scaled)[:,1]\n",
    "\n",
    "    # Calculate prediction scores and add the dictionary to the list\n",
    "    score = selectors.getScores(y_test, y_pred)\n",
    "    score['checkpoint'] = i*2\n",
    "    scores.append(score)\n",
    "except:\n",
    "  None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display scores of each checkpoint\n",
    "scores = pd.DataFrame(scores).sort_values(by='AUC', ascending=False)\n",
    "scores.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs421",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
