{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Select path to current folder and split by \\\\\n",
    "main_path = sys.path[0].split(\"\\\\\")\n",
    "\n",
    "# Asssign path to parent folder\n",
    "# path_to_parent allows access to any folder from within parent folder, no matter the location of this file within the parent folder\n",
    "# i.e.: Don't need to specify \"../\" x amount of times\n",
    "path_to_parent = []\n",
    "for element in main_path:\n",
    "    path_to_parent.append(element)\n",
    "    if \"Fake_Users_Movies_Classifier\" == element:\n",
    "        break\n",
    "\n",
    "path_to_parent = \"\\\\\".join(path_to_parent)\n",
    "\n",
    "# Add path to feature generation folder\n",
    "sys.path.append(path_to_parent+\"\\\\feature_generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier, Lars\n",
    "from sklearn.linear_model import LinearRegression, Ridge, OrthogonalMatchingPursuit, LassoLarsCV, LassoLarsIC\n",
    "from sklearn.linear_model import ARDRegression, BayesianRidge, PoissonRegressor, TweedieRegressor, GammaRegressor\n",
    "from sklearn.linear_model import HuberRegressor, TheilSenRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create a list of classifier names\n",
    "names = [\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "    \"Gaussian Process\",\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"Logistic Regression\",\n",
    "    \"Ridge Classifier\",\n",
    "    \"SGD Classifier\",\n",
    "    \"Lars\",\n",
    "    \"Linear Regression\",\n",
    "    \"Ridge\",\n",
    "    \"Orthogonal Matching Pursuit\",\n",
    "    \"LassoLars CV\",\n",
    "    \"LassoLars IC\",\n",
    "    \"ARD Regression\",\n",
    "    \"Bayesian Ridge\",\n",
    "    \"Huber Regressor\",\n",
    "    \"Theil Sen Regressor\",\n",
    "    \"Poisson Regressor\",\n",
    "    \"Tweedie Regressor\",\n",
    "]\n",
    "\n",
    "# Create a list of classifiers\n",
    "classifiers = [\n",
    "    SVC(kernel=\"linear\", C=0.025, random_state=42),\n",
    "    SVC(gamma=2, C=1, random_state=42),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0), random_state=42),\n",
    "    MLPClassifier(alpha=1, max_iter=1000, random_state=42),\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    GaussianNB(),\n",
    "    LogisticRegression(max_iter=10000, random_state=42),\n",
    "    RidgeClassifier(max_iter=10000, random_state=42),\n",
    "    SGDClassifier(max_iter=10000, random_state=42),\n",
    "    Lars(random_state=42),\n",
    "    LinearRegression(),\n",
    "    Ridge(random_state=42),\n",
    "    OrthogonalMatchingPursuit(),\n",
    "    LassoLarsCV(),\n",
    "    LassoLarsIC(),\n",
    "    ARDRegression(),\n",
    "    BayesianRidge(),\n",
    "    HuberRegressor(),\n",
    "    TheilSenRegressor(),\n",
    "    PoissonRegressor(),\n",
    "    TweedieRegressor(),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import feature generator for week 1\n",
    "from feature_gen_wk1 import feature_gen\n",
    "\n",
    "# Create string path to labelled data\n",
    "path_to_file = path_to_parent + \"/data/labelled_data/first_batch_with_labels_likes.npz\"\n",
    "# Generate features from file\n",
    "df_final = feature_gen().retrieveAndGenerate(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from assessors import assessFeatures\n",
    "\n",
    "hist = []\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Retrieve labels and assign to y\n",
    "y = df_final['label']\n",
    "# Remove labels and users from dataset and assign to x\n",
    "X = df_final.drop(['user','label'],axis=1)\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Initialize feature selection class\n",
    "assessor = assessFeatures(X_train, y_train)\n",
    "\n",
    "# Loop through amount of columns: from 1 to column count - 2\n",
    "for i in range(1,len(X_train.columns)-2):\n",
    "    # Create a dictionary with classifier scores and best score achieved\n",
    "    hist_inner = {i:[], \"best_score\":0}\n",
    "    \n",
    "    # Apply RFE feature selection algorithm to train set and return modified train set\n",
    "    # Feature amount output is denoted by the value i\n",
    "    # Additional Information:\n",
    "    #       This algorithm can be changed to test other ones such as:\n",
    "    #               -boruta\n",
    "    #               -chi2\n",
    "    #               -ANOVA\n",
    "    best_features = assessor.select_by_RFE(classifier, i)\n",
    "\n",
    "    # Find columns used in modified train set\n",
    "    columns = set(best_features.columns)&set(X_train.columns)\n",
    "\n",
    "    # Assign train and test sets with new features\n",
    "    X_train_inner = best_features\n",
    "    X_test_inner = X_test[columns]\n",
    "\n",
    "    # Iterate over classifiers and calculate scores\n",
    "    for name, clf in zip(names, classifiers):\n",
    "\n",
    "        # Create a pipeline to scale input data before training on classifier\n",
    "        clf = make_pipeline(StandardScaler(), clf)\n",
    "        # Train classifier on train data\n",
    "        clf.fit(X_train_inner, y_train)\n",
    "\n",
    "        # Predict test labels and calculate AUC score\n",
    "        y_pred = clf.predict(X_test_inner)\n",
    "        score = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "        # Append AUC score to list inside hist_inner dictionary\n",
    "        hist_inner[i].append({'clf':name, 'auc':score})\n",
    "\n",
    "        # Check if score is highest in current set of classifiers\n",
    "        if score > hist_inner['best_score']:\n",
    "            # If yes, set it as the highest\n",
    "            hist_inner['best_score'] = score\n",
    "\n",
    "    # Append performance of classifiers for this iteration to list\n",
    "    hist.append(hist_inner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert list into dataframe and sort by best_score\n",
    "results_table = pd.DataFrame(hist).sort_values(by='best_score', ascending=False)\n",
    "# Display results dataframe\n",
    "results_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs421",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
