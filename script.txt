At the start of this project, we noticed that the dataset provided was heavily unbalanced, a 20/80 split of the labels. This is a problem when training a model, as it may result with one of the labels being more favorized; therefore, to help alleviate this problem, we resorted to applying class weights, when working with neural network models. Overall, we saw that the model’s performance improved by approximately 0.02 in AUC, however this is not to be said that it’s ineffective. The test set used was similarly imbalanced, so the difference in performance, may not be as visible, however, it could be more pronounced on a more balanced set, or one which is imbalanced towards the other label.

Another key finding, or more of a proof of concept, is the model architecture itself. Prior to the final model, we have experimented with slightly larger models; these models weren’t performing poorly, and after a bit more research, we were able to discover and implement a few more features into the dataset; with these new features, our model performed slightly better; however we discovered that decreasing the layers in the model, significantly improved performance. This goes to show how a more complex model may perform worse compared to a simpler model.

We also tested the performance of our models with different activation functions, mainly Selu, Gelu and Relu. During model development, we found Selu to perform the best with feature set that we had closer to the beginning of this project. Moving onwards, Gelu by itself didn’t show any sign of improvement, performing worse than selu; although this changed with the introduction of PCA. We noticed that a model with the GELU activation function benefited greatly from PCA, scoring higher than the one with SELU, on the flip side the SELU model decreased in performance, once PCA was applied. Finally, once we downsized the model and introduced new features, we found that the model with RELU activation and without PCA, performed the best.

Thank you very much for you attention.
